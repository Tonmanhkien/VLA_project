{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6fdd2e",
   "metadata": {},
   "source": [
    "# VLA Behavior Cloning Demo\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Collecting expert data using an expert policy\n",
    "2. Training a VLA (Vision-Language Action) model with Behavior Cloning\n",
    "3. Running inference and generating a video\n",
    "\n",
    "The key issue fixed from the original demo: the model is now **properly trained** with backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68936de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q gymnasium imageio[ffmpeg] torch torchvision transformers accelerate sentencepiece tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f6b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel, DistilBertModel, DistilBertTokenizer\n",
    "from PIL import Image\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Reproducibility ---\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e7803",
   "metadata": {},
   "source": [
    "## 1. Expert Policy\n",
    "\n",
    "CartPole state: `[x, x_dot, theta, theta_dot]`\n",
    "\n",
    "The expert policy: if pole tilts right, push right; if tilts left, push left. Simple but effective for generating training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a86b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_action(obs):\n",
    "    # obs: [x, x_dot, theta, theta_dot]\n",
    "    x, x_dot, theta, theta_dot = obs\n",
    "    # Linear controller \n",
    "    score = theta + 0.5 * theta_dot + 0.05 * x + 0.1 * x_dot\n",
    "    return 1 if score > 0 else 0  # 0=left, 1=right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ca88f",
   "metadata": {},
   "source": [
    "## 2. Data Collection\n",
    "\n",
    "Collect RGB frames and expert actions. Using a fixed instruction for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e05b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data collection params ---\n",
    "N_EPISODES = 30\n",
    "MAX_STEPS_PER_EP = 500\n",
    "INSTRUCTION = \"Keep the pole balanced\"\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "frames, actions = [], []\n",
    "episode_lengths = []\n",
    "\n",
    "for ep in tqdm(range(N_EPISODES), desc=\"Collect expert data\"):\n",
    "    obs, _ = env.reset(seed=SEED + ep)\n",
    "    for t in range(MAX_STEPS_PER_EP):\n",
    "        frame = env.render()\n",
    "        a = expert_action(obs)\n",
    "\n",
    "        obs, reward, done, trunc, info = env.step(a)\n",
    "\n",
    "        frames.append(frame)\n",
    "        actions.append(a)\n",
    "\n",
    "        if done or trunc:\n",
    "            episode_lengths.append(t + 1)\n",
    "            break\n",
    "    else:\n",
    "        episode_lengths.append(MAX_STEPS_PER_EP)\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"Collected frames:\", len(frames))\n",
    "print(\"Avg episode length (expert):\", np.mean(episode_lengths))\n",
    "print(\"Action balance:\", {0: int(np.sum(np.array(actions)==0)), 1: int(np.sum(np.array(actions)==1))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f70e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick peek\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.imshow(frames[0])\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Example frame\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9cebee",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader\n",
    "\n",
    "Create a dataset with frames, actions, and instruction. Split into 90% train, 10% validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbebebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLADataset(Dataset):\n",
    "    def __init__(self, frames, actions, instruction):\n",
    "        self.frames = frames\n",
    "        self.actions = actions\n",
    "        self.instruction = instruction\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.frames[idx], self.instruction, int(self.actions[idx])\n",
    "\n",
    "# Train/val split\n",
    "idx = np.arange(len(frames))\n",
    "np.random.shuffle(idx)\n",
    "split = int(0.9 * len(idx))\n",
    "train_idx, val_idx = idx[:split], idx[split:]\n",
    "\n",
    "train_ds = VLADataset([frames[i] for i in train_idx], [actions[i] for i in train_idx], INSTRUCTION)\n",
    "val_ds   = VLADataset([frames[i] for i in val_idx],   [actions[i] for i in val_idx],   INSTRUCTION)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(len(train_ds), len(val_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc27d1",
   "metadata": {},
   "source": [
    "## 4. MiniVLAAgent\n",
    "\n",
    "Freeze CLIP (vision) + DistilBERT (text) encoders. Train fusion layer + policy head only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ae4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniVLAAgent(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "        self.text_encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        # Freeze encoders\n",
    "        for p in self.clip.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.text_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.fuse = nn.Linear(self.clip.config.projection_dim + self.text_encoder.config.dim, 256)\n",
    "        self.policy_head = nn.Linear(256, action_dim)\n",
    "\n",
    "    def forward(self, image_inputs, text_inputs):\n",
    "        # encoders frozen â†’ compute embeddings without grad\n",
    "        with torch.no_grad():\n",
    "            vision_emb = self.clip.get_image_features(**image_inputs)\n",
    "            text_emb = self.text_encoder(**text_inputs).last_hidden_state[:, 0, :]\n",
    "\n",
    "        fused = torch.relu(self.fuse(torch.cat([vision_emb, text_emb], dim=-1)))\n",
    "        logits = self.policy_head(fused)\n",
    "        return logits\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "text_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Model\n",
    "env_tmp = gym.make(\"CartPole-v1\")\n",
    "n_actions = env_tmp.action_space.n\n",
    "env_tmp.close()\n",
    "\n",
    "agent = MiniVLAAgent(n_actions).to(device)\n",
    "agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5272dd6",
   "metadata": {},
   "source": [
    "## 5. Behavior Cloning Training\n",
    "\n",
    "Use cross-entropy loss to train the model to match expert actions. Run 4 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37fb793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    frames_b, instr_b, act_b = zip(*batch)\n",
    "\n",
    "    image_inputs = clip_processor(\n",
    "        images=[Image.fromarray(f) for f in frames_b],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    text_inputs = text_tokenizer(\n",
    "        list(instr_b),\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    actions_t = torch.tensor(act_b, dtype=torch.long)\n",
    "\n",
    "    return image_inputs, text_inputs, actions_t\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(list(agent.fuse.parameters()) + list(agent.policy_head.parameters()), lr=3e-4)\n",
    "\n",
    "def evaluate(loader):\n",
    "    agent.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for image_inputs, text_inputs, actions_t in loader:\n",
    "            image_inputs = {k:v.to(device) for k,v in image_inputs.items()}\n",
    "            text_inputs  = {k:v.to(device) for k,v in text_inputs.items()}\n",
    "            actions_t    = actions_t.to(device)\n",
    "\n",
    "            logits = agent(image_inputs, text_inputs)\n",
    "            loss = criterion(logits, actions_t)\n",
    "            total_loss += loss.item() * actions_t.size(0)\n",
    "\n",
    "            pred = torch.argmax(logits, dim=-1)\n",
    "            correct += (pred == actions_t).sum().item()\n",
    "            total += actions_t.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "EPOCHS = 4\n",
    "train_losses, val_losses, val_accs = [], [], []\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    agent.train()\n",
    "    running = 0.0\n",
    "    n = 0\n",
    "    for image_inputs, text_inputs, actions_t in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\"):\n",
    "        image_inputs = {k:v.to(device) for k,v in image_inputs.items()}\n",
    "        text_inputs  = {k:v.to(device) for k,v in text_inputs.items()}\n",
    "        actions_t    = actions_t.to(device)\n",
    "\n",
    "        logits = agent(image_inputs, text_inputs)\n",
    "        loss = criterion(logits, actions_t)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += loss.item() * actions_t.size(0)\n",
    "        n += actions_t.size(0)\n",
    "\n",
    "    tr_loss = running / n\n",
    "    va_loss, va_acc = evaluate(val_loader)\n",
    "\n",
    "    train_losses.append(tr_loss)\n",
    "    val_losses.append(va_loss)\n",
    "    val_accs.append(va_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch}: train_loss={tr_loss:.4f} | val_loss={va_loss:.4f} | val_acc={va_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"train_loss\")\n",
    "plt.plot(val_losses, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Behavior Cloning loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_accs, label=\"val_acc\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3648ac41",
   "metadata": {},
   "source": [
    "## 6. Inference & Video Generation\n",
    "\n",
    "Run the trained policy on a new episode and save as video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_action(frame, instruction, temperature=1.0, sample=False):\n",
    "    image_inputs = clip_processor(images=Image.fromarray(frame), return_tensors=\"pt\")\n",
    "    text_inputs  = text_tokenizer(instruction, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    image_inputs = {k:v.to(device) for k,v in image_inputs.items()}\n",
    "    text_inputs  = {k:v.to(device) for k,v in text_inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = agent(image_inputs, text_inputs)[0]  # (action_dim,)\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "        if sample:\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            return int(dist.sample().item())\n",
    "        else:\n",
    "            return int(torch.argmax(logits).item())\n",
    "\n",
    "def run_episode(max_steps=1000, sample=False):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    obs, _ = env.reset(seed=SEED + 999)\n",
    "    frames_out = []\n",
    "    for t in range(max_steps):\n",
    "        frame = env.render()\n",
    "        a = policy_action(frame, INSTRUCTION, temperature=1.0, sample=sample)\n",
    "        obs, reward, done, trunc, info = env.step(a)\n",
    "        frames_out.append(frame)\n",
    "        if done or trunc:\n",
    "            break\n",
    "    env.close()\n",
    "    return frames_out, t+1\n",
    "\n",
    "# Run\n",
    "frames_out, steps = run_episode(sample=False)\n",
    "print(\"Episode length:\", steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d93eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save video\n",
    "video_path = \"vla_cartpole_bc.mp4\"\n",
    "imageio.mimsave(video_path, frames_out, fps=30)\n",
    "video_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display video (Colab/Jupyter)\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "with open(video_path, \"rb\") as f:\n",
    "    mp4 = b64encode(f.read()).decode()\n",
    "\n",
    "HTML(f'''\n",
    "<video width=\"640\" controls>\n",
    "  <source src=\"data:video/mp4;base64,{mp4}\" type=\"video/mp4\">\n",
    "</video>\n",
    "''')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
